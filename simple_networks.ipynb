{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Импорт библиотек"
      ],
      "metadata": {
        "id": "E9IBY2kOd0cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "7QD5fGDugMdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8q3uXmEdx4x"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pymorphy2\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "iiPILWpEsJI7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Импорт данных"
      ],
      "metadata": {
        "id": "efZ-2Tn6d3xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = pd.read_csv('Data_ASR_2.csv')"
      ],
      "metadata": {
        "id": "z7OKX8P3ekn1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_sentiment_label(row):\n",
        "  if row['sentiment'] > 0:\n",
        "    return 1\n",
        "  elif row['sentiment'] < 0:\n",
        "    return 2 # чтобы не было -1 в лейблах\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "csv_data['sentiment_label'] = csv_data.apply(calc_sentiment_label, axis=1)"
      ],
      "metadata": {
        "id": "GaXylRU7kR9X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = csv_data['sentiment_label'].tolist()"
      ],
      "metadata": {
        "id": "deyqI7Z_lDZV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data.sample(3)"
      ],
      "metadata": {
        "id": "b-ncGIKDhf89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data.shape"
      ],
      "metadata": {
        "id": "yRAVM1WGsgpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Текстовые данные"
      ],
      "metadata": {
        "id": "IaTQyLbdd40E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Препроцессинг"
      ],
      "metadata": {
        "id": "DTRFGZUYf-Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = csv_data['text'].tolist()\n",
        "text_data[0]"
      ],
      "metadata": {
        "id": "gHdODLWvhe_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(string):\n",
        "  result = word_tokenize(string)\n",
        "\n",
        "  punctiations = list(punctuation)\n",
        "  result = [i for i in result if (i not in punctiations)]\n",
        "\n",
        "  result =  [i.lower() for i in result]\n",
        "\n",
        "  stop_words = nltk.corpus.stopwords.words('english')\n",
        "  result = [i for i in result if ( i not in stop_words )]\n",
        "\n",
        "  wnl = WordNetLemmatizer()\n",
        "  result = [wnl.lemmatize(word, pos=\"v\") for word in result]\n",
        "\n",
        "  return ' '.join(result)"
      ],
      "metadata": {
        "id": "J-C0TRUvgAnA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text_data = [preprocess(string) for string in text_data]\n",
        "preprocessed_text_data[0]"
      ],
      "metadata": {
        "id": "aCTeUgpThw-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Извлечение признаков"
      ],
      "metadata": {
        "id": "6Zwrh9khd9ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(max_features=10000)\n",
        "bow_X = vectorizer.fit_transform(np.asarray(preprocessed_text_data)).toarray()"
      ],
      "metadata": {
        "id": "bezkymYfj7Mn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка к обучению"
      ],
      "metadata": {
        "id": "MH8MTQDQd-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(bow_X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Bya-C-3IlgPp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Нейронная сеть"
      ],
      "metadata": {
        "id": "rkJU2W2hV1u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TFIDFClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, num_tokens, embedding_dim, num_filters, filter_sizes):\n",
        "        super(TFIDFClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_tokens, embedding_dim)\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        conv_outputs = [F.relu(conv(x)) for conv in self.conv_layers]\n",
        "        pooled_outputs = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in conv_outputs]\n",
        "        x = torch.cat(pooled_outputs, dim=1)\n",
        "        x = self.fc(x)\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, num_epochs, learning_rate=0.001):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=0.001)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_f1s = []\n",
        "        val_f1s = []\n",
        "        train_labels_all = []\n",
        "        train_outputs_all = []\n",
        "        test_labels_all = []\n",
        "        test_outputs_all = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.train()\n",
        "            running_train_loss = 0.0\n",
        "            correct_train = 0\n",
        "            total_train = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_train_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_train += labels.size(0)\n",
        "                correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "                train_labels_all.extend(labels.tolist())\n",
        "                train_outputs_all.extend(predicted.tolist())\n",
        "\n",
        "            train_loss = running_train_loss / len(train_loader)\n",
        "            train_losses.append(train_loss)\n",
        "            train_f1s.append(f1_score(train_labels_all, train_outputs_all, average='weighted'))\n",
        "\n",
        "            self.eval()\n",
        "            running_val_loss = 0.0\n",
        "            correct_val = 0\n",
        "            total_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    outputs = self(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    running_val_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    total_val += labels.size(0)\n",
        "                    correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "                    test_labels_all.extend(labels.tolist())\n",
        "                    test_outputs_all.extend(predicted.tolist())\n",
        "\n",
        "                val_loss = running_val_loss / len(val_loader)\n",
        "                val_losses.append(val_loss)\n",
        "                val_f1s.append(f1_score(test_labels_all, test_outputs_all, average='weighted'))\n",
        "\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        return train_losses, val_losses, train_f1s, val_f1s\n"
      ],
      "metadata": {
        "id": "RF3IxsgSca-0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.long).cuda()\n",
        "y_train = torch.tensor(y_train).cuda()\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.long).cuda()\n",
        "y_test = torch.tensor(y_test).cuda()\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset =  TensorDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "num_classes = 3\n",
        "num_tokens = X_train.shape[1]\n",
        "embedding_dim = 100\n",
        "num_filters = 128\n",
        "filter_sizes = [3, 4, 5]\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "_uUVB8VXcthB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFIDFClassifier(num_classes, num_tokens, embedding_dim, num_filters, filter_sizes)\n",
        "model.cuda()\n",
        "train_losses, val_losses, train_f1s, val_f1s = model.train_model(train_loader, val_loader, num_epochs)"
      ],
      "metadata": {
        "id": "jgnLeeYRd3Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Результаты"
      ],
      "metadata": {
        "id": "JcrnWMNWmMyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label='Функция потерь на обучающей выборке')\n",
        "plt.plot(epochs, val_losses, label='Функция потерь на валидационной выборке')\n",
        "plt.title('Кривая функции потерь')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('Функция потерь')\n",
        "plt.legend(loc = \"upper left\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_f1s, label='F1 на обучающей выборке')\n",
        "plt.plot(epochs, val_f1s, label='F1 на валидационной выборке')\n",
        "plt.title('Кривая F1')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('F1')\n",
        "plt.legend(loc = \"upper left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H111z2o-8ho6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Аудио данные"
      ],
      "metadata": {
        "id": "mRb_nziod7x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/masters/ML/Audio.zip'"
      ],
      "metadata": {
        "id": "lw7bMcqKfQYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames = csv_data[['video', 'start_time', 'end_time']]\n",
        "filenames.head(3)"
      ],
      "metadata": {
        "id": "ye1S1fkoUKGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Извлечение признаков"
      ],
      "metadata": {
        "id": "iAh6VXxWeGtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MFCC"
      ],
      "metadata": {
        "id": "tGBHbcgLebcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mfcc(row, n_mfcc=20, hop_length=512, maxlen=300):\n",
        "    audio_path = row['video']\n",
        "    audio_path = '/content/Audio/WAV_16000/' + audio_path + '.wav'\n",
        "\n",
        "    start_time = row['start_time']\n",
        "    end_time = row['end_time']\n",
        "    duration = end_time - start_time\n",
        "    audio, sr = librosa.load(audio_path, offset=start_time, duration=duration, sr=None)\n",
        "\n",
        "    mfcc_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
        "\n",
        "    # приводим к одном длине\n",
        "    if maxlen and mfcc_features.shape[1] > maxlen:\n",
        "      mfcc_features = mfcc_features[:, :maxlen]\n",
        "    elif maxlen and mfcc_features.shape[1] < maxlen:\n",
        "      mfcc_features = np.pad(mfcc_features, ((0, 0), (0, maxlen - mfcc_features.shape[1]) ))\n",
        "    return mfcc_features"
      ],
      "metadata": {
        "id": "ZSWD4Vseekxh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc_features = []\n",
        "for index, row in filenames.iterrows():\n",
        "  mfcc_features.append(calculate_mfcc(row))"
      ],
      "metadata": {
        "id": "2I_a2hF9hanM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка к обучению"
      ],
      "metadata": {
        "id": "UGkuIDguaP-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(mfcc_features, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "T5I3mvyXZEGo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Нейронная сеть"
      ],
      "metadata": {
        "id": "Rn7lFEgLTwQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MFCC_Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_layers=(128, 128, 64), dropout_rate=0.2):\n",
        "      super(MFCC_Classifier, self).__init__()\n",
        "      self.input_dim = input_dim\n",
        "      self.output_dim = output_dim\n",
        "      self.hidden_layers = hidden_layers\n",
        "      self.dropout_rate = dropout_rate\n",
        "\n",
        "      layers = []\n",
        "      prev_dim = input_dim\n",
        "      for units in hidden_layers:\n",
        "          layers.append(nn.Linear(prev_dim, units))\n",
        "          layers.append(nn.ReLU())\n",
        "          layers.append(nn.Dropout(dropout_rate))\n",
        "          prev_dim = units\n",
        "      layers.append(nn.Linear(prev_dim, output_dim))\n",
        "\n",
        "      self.model = nn.Sequential(*layers)\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.model(x)\n",
        "      x = self.softmax(x)\n",
        "      return x\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, num_epochs, learning_rate=0.001):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=learning_rate, weight_decay=0.001)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_f1s = []\n",
        "        val_f1s = []\n",
        "        train_labels_all = []\n",
        "        train_outputs_all = []\n",
        "        test_labels_all = []\n",
        "        test_outputs_all = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            self.train()\n",
        "            running_train_loss = 0.0\n",
        "            correct_train = 0\n",
        "            total_train = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                predicted = self(inputs)\n",
        "                loss = criterion(predicted, labels.unsqueeze(1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_train_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(predicted, 1)\n",
        "                total_train += labels.size(0)\n",
        "                correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "                train_labels_all.extend(labels.tolist())\n",
        "                train_outputs_all.extend(predicted.tolist())\n",
        "\n",
        "            train_loss = running_train_loss / len(train_loader)\n",
        "            train_losses.append(train_loss)\n",
        "            train_f1s.append(f1_score(train_labels_all, train_outputs_all, average='weighted'))\n",
        "\n",
        "            self.eval()\n",
        "            running_val_loss = 0.0\n",
        "            correct_val = 0\n",
        "            total_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    predicted = self(inputs)\n",
        "                    loss = criterion(predicted, labels.unsqueeze(1))\n",
        "\n",
        "                    running_val_loss += loss.item()\n",
        "                    _, predicted = torch.max(predicted, 1)\n",
        "                    total_val += labels.size(0)\n",
        "                    correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "                    test_labels_all.extend(labels.tolist())\n",
        "                    test_outputs_all.extend(predicted.tolist())\n",
        "\n",
        "                val_loss = running_val_loss / len(val_loader)\n",
        "                val_losses.append(val_loss)\n",
        "                val_f1s.append(f1_score(test_labels_all, test_outputs_all, average='weighted'))\n",
        "\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        return train_losses, val_losses, train_f1s, val_f1s\n"
      ],
      "metadata": {
        "id": "b9uBsTcRWwhG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(np.array(X_train), dtype=torch.float32).cuda()\n",
        "y_train = torch.tensor(y_train).cuda()\n",
        "\n",
        "X_test = torch.tensor(np.array(X_test), dtype=torch.float32).cuda()\n",
        "y_test = torch.tensor(y_test).cuda()\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset =  TensorDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "PbSoB0RqMzOs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=50"
      ],
      "metadata": {
        "id": "gvXCqzUymTJl"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MFCC_Classifier(input_dim=X_train.shape[2], output_dim=1)\n",
        "model.cuda()\n",
        "train_losses, val_losses, train_f1s, val_f1s = model.train_model(train_loader, val_loader, num_epochs)"
      ],
      "metadata": {
        "id": "Z9TMkK0do8Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Результаты"
      ],
      "metadata": {
        "id": "Glem29p3Txr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label='Функция потерь на обучающей выборке')\n",
        "plt.plot(epochs, val_losses, label='Функция потерь на валидационной выборке')\n",
        "plt.title('Кривая функции потерь')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('Функция потерь')\n",
        "plt.legend(loc = \"upper left\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_f1s, label='F1 на обучающей выборке')\n",
        "plt.plot(epochs, val_f1s, label='F1 на валидационной выборке')\n",
        "plt.title('Кривая F1')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.ylabel('F1')\n",
        "plt.legend(loc = \"upper left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s-a1DXLaOLtJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}